{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1IOXlYqpSNINwJ1Gv9qDa5B7Z9qmQh4jp",
      "authorship_tag": "ABX9TyO4LTGJe0UvG4wmWHBWKZ2M"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9OIW2ZtWTad"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/MSc Dissertation/Tescobank reviews.csv')\n",
        "import pandas as pd\n",
        "\n",
        "# Annotated dataset for 3-star reviews\n",
        "df_annotations = pd.read_csv('/content/drive/MyDrive/MSc Dissertation/Annotated tesco1.csv')  # should have columns like: review_text, annotated_sentiment\n",
        "\n",
        "\n",
        "# Inspect the data\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotations.head()"
      ],
      "metadata": {
        "id": "ePbDn3EybQep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Review Date'] = df['Review Date'].str.replace('Date of experience: ', '').str.strip()\n",
        "df['Star Rating'] = df['Star Rating'].str.extract(r'(\\d+)').astype(int)"
      ],
      "metadata": {
        "id": "MizVlnxxXGmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Review Date'] = pd.to_datetime(df['Review Date'], errors='coerce')"
      ],
      "metadata": {
        "id": "VS3RgfJbXHKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotations['Date'] = pd.to_datetime(df_annotations['Date'], errors='coerce')"
      ],
      "metadata": {
        "id": "a5rW9CkrczzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['Review Message'])"
      ],
      "metadata": {
        "id": "hDilZdArYOL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotations  = df_annotations.dropna(subset=['Review message'])"
      ],
      "metadata": {
        "id": "pm6kOXaubaAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "tcxxqkuPXLGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotations  = df_annotations.drop_duplicates()"
      ],
      "metadata": {
        "id": "AuGzmb_ybv0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "f5n9smoMXYGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotations.info()"
      ],
      "metadata": {
        "id": "NX7-pYX0b-pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Author Name'] = df['Author Name'].fillna('Anonymous')"
      ],
      "metadata": {
        "id": "P3TvZB8KXgM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = df[df['Review Date'].dt.year.between(2020, 2025)]"
      ],
      "metadata": {
        "id": "irfe9NLzzB_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = df_annotations[df_annotations['Date'].dt.year.between(2020, 2025)]"
      ],
      "metadata": {
        "id": "bUByZbFjcgQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "IqIectANXssv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df['Star Rating'].value_counts().sort_index().plot(\n",
        "    kind='bar', color='skyblue', edgecolor='black'\n",
        ")\n",
        "plt.title('Tesco Bank - Distribution of Star Ratings')\n",
        "plt.xlabel('Star Rating')\n",
        "plt.ylabel('Count of Reviews')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z1cmvBmubemO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rate_to_sentiment(star):\n",
        "    if star <= 2:\n",
        "        return 'Negative'\n",
        "    elif star == 3:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Positive'\n",
        "\n",
        "df['Sentiment Label'] = df['Star Rating'].apply(rate_to_sentiment)\n",
        "\n"
      ],
      "metadata": {
        "id": "2I8ybKlCcyP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot sentiment label distribution\n",
        "df['Sentiment Label'].value_counts().plot(\n",
        "    kind='bar',\n",
        "    color=['green', 'red', 'grey'],  # red = Negative, gray = Neutral, green = Positive\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.title('Tesco Bank Sentiment Distribution from Star Ratings')\n",
        "plt.xlabel('Sentiment Label')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "An2vRDnbc4yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "onqjI50Xa30_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
      ],
      "metadata": {
        "id": "oMGtrg5fbxHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy\n",
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "ZEW128v_yela"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "Q8u12RgoZfI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning function\n",
        "def clean_text(text):\n",
        "    doc = nlp(str(text).lower())  # Ensure text is string\n",
        "    return [\n",
        "        token.lemma_ for token in doc\n",
        "        if token.is_alpha and not token.is_stop and len(token) > 2\n",
        "    ]\n",
        "\n",
        "# Apply cleaning\n",
        "df['cleaned_text'] = df['Review Message'].apply(clean_text)\n",
        "\n",
        "# Inspect cleaned data\n",
        "print(df[['Review Message', 'cleaned_text']].head())\n"
      ],
      "metadata": {
        "id": "6xPPnwLTYWJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "fOYUNYlBdOJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy==1.13.1\n"
      ],
      "metadata": {
        "id": "WyqjlUj_Q_zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --force-reinstall gensim"
      ],
      "metadata": {
        "id": "UnYjhCjrRE4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "print(gensim.__version__)"
      ],
      "metadata": {
        "id": "FghpU5WTdRKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.phrases import Phrases, Phraser"
      ],
      "metadata": {
        "id": "s7QFdYDMdXL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build bigram model\n",
        "bigram = Phrases(df['cleaned_text'], min_count=5, threshold=10)\n",
        "bigram_mod = Phraser(bigram)\n",
        "\n",
        "# Apply bigram model\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda text: bigram_mod[text])"
      ],
      "metadata": {
        "id": "Tk_ifh0_dhQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Create dictionary\n",
        "id2word = Dictionary(df['cleaned_text'])\n",
        "\n",
        "# Filter extremes (optional, for better topic quality)\n",
        "id2word.filter_extremes(no_below=5, no_above=0.6)\n",
        "\n",
        "# Create corpus (bag of words)\n",
        "corpus = [id2word.doc2bow(text) for text in df['cleaned_text']]\n"
      ],
      "metadata": {
        "id": "ZE6Sx48QdqJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "# Train LDA model\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=5,         # Tune this value based on coherence score\n",
        "    random_state=100,\n",
        "    update_every=1,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "# Print the topics\n",
        "for idx, topic in lda_model.print_topics():\n",
        "    print(f\"Topic {idx}: {topic}\\n\")\n"
      ],
      "metadata": {
        "id": "fw_d2zrldwqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Calculate coherence score\n",
        "coherence_model_lda = CoherenceModel(\n",
        "    model=lda_model,\n",
        "    texts=df['cleaned_text'],\n",
        "    dictionary=id2word,\n",
        "    coherence='c_v'\n",
        ")\n",
        "\n",
        "coherence_score = coherence_model_lda.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score}\")\n"
      ],
      "metadata": {
        "id": "ICYHL4JmecYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels = {\n",
        "    0: \"Insurance Claim Issues\",\n",
        "    1: \"Account & Customer Support\",\n",
        "    2: \"Credit Card Issues\",\n",
        "    3: \"Customer Service Quality\",\n",
        "    4: \"Insurance Coverage & Experience\"\n",
        "}"
      ],
      "metadata": {
        "id": "kThV-1wjI6PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model.print_topics():\n",
        "    print(f\"Topic {idx} - {topic_labels[idx]}: {topic}\\n\")\n"
      ],
      "metadata": {
        "id": "it3v6K4QI70h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H3JdpBbIJANS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "0gM4m33_CA03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Apply sentiment analysis to each review\n",
        "df[\"Sentiment Score\"] = df[\"Review Message\"].apply(lambda text: sia.polarity_scores(text)[\"compound\"])\n",
        "\n",
        "# Assign topics to each review\n",
        "df[\"Topic\"] = [lda_model.get_document_topics(corpus[i], minimum_probability=0.3) for i in range(len(df))]\n",
        "\n",
        "# Extract dominant topic per review and map it to topic name\n",
        "df[\"Dominant Topic\"] = df[\"Topic\"].apply(lambda topics: topic_labels[max(topics, key=lambda x: x[1])[0]] if topics else None)\n",
        "\n",
        "# Get average sentiment scores per topic\n",
        "sentiment_by_topic = df.groupby(\"Dominant Topic\")[\"Sentiment Score\"].mean()\n",
        "\n",
        "print(\"\\n✅ Average Sentiment Score per Topic:\")\n",
        "print(sentiment_by_topic)"
      ],
      "metadata": {
        "id": "LRnu8En6B0xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sort sentiment scores for better visualization\n",
        "sentiment_by_topic = sentiment_by_topic.sort_values(ascending=True)\n",
        "\n",
        "# Create a horizontal bar chart with all bars in green\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=sentiment_by_topic.values, y=sentiment_by_topic.index, color=\"green\")\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Average Sentiment Score\")\n",
        "plt.ylabel(\"Topic\")\n",
        "plt.title(\"Sentiment Score per Topic\")\n",
        "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)  # Vertical line at 0\n",
        "\n",
        "# Remove legend (not needed when all bars are green)\n",
        "plt.legend([],[], frameon=False)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CLaq5nPMC7Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of reviews per topic\n",
        "topic_distribution = df[\"Dominant Topic\"].value_counts()\n",
        "\n",
        "# Create a horizontal bar chart for topic distribution with blue bars\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=topic_distribution.values, y=topic_distribution.index, color=\"blue\")\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Number of Reviews\")\n",
        "plt.ylabel(\"Topic\")\n",
        "plt.title(\"Distribution of Topics\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RFH22d_TOspr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# Combine all reviews into a single string\n",
        "all_reviews_text = \" \".join(df[\"Review Message\"].astype(str))\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\", colormap=\"viridis\").generate(all_reviews_text)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")  # Hide axes for better visualization\n",
        "plt.title(\"Word Cloud of All Reviews - Tesco Bank\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zMv1G-XdPNkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sentiment categories\n",
        "df[\"Sentiment Category\"] = df[\"Sentiment Score\"].apply(lambda x: \"Positive\" if x > 0.05 else (\"Negative\" if x < -0.05 else \"Neutral\"))\n",
        "\n",
        "# Calculate sentiment distribution per topic\n",
        "sentiment_distribution = df.groupby(\"Dominant Topic\")[\"Sentiment Category\"].value_counts(normalize=True).unstack()\n",
        "\n",
        "# Create a horizontal stacked bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sentiment_distribution.plot(kind=\"barh\", stacked=True, colormap=\"coolwarm\", alpha=0.85, figsize=(12, 6))\n",
        "\n",
        "# Labels and title\n",
        "plt.ylabel(\"Topic\", fontsize=12)\n",
        "plt.xlabel(\"Proportion of Sentiment\", fontsize=12)\n",
        "plt.title(\"Sentiment Distribution by Topics\", fontsize=14)\n",
        "plt.xticks(rotation=0)  # Keep x-axis labels horizontal\n",
        "plt.legend(title=\"Sentiment Category\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H4entNicSw97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sentiment categories\n",
        "df[\"Sentiment Category\"] = df[\"Sentiment Score\"].apply(lambda x: \"Positive\" if x > 0.05 else (\"Negative\" if x < -0.05 else \"Neutral\"))\n",
        "\n",
        "# Calculate sentiment distribution per topic\n",
        "sentiment_distribution = df.groupby(\"Dominant Topic\")[\"Sentiment Category\"].value_counts(normalize=True).unstack()\n",
        "\n",
        "# Define custom color mapping: Negative = red, Neutral = grey, Positive = blue\n",
        "custom_colors = {\n",
        "    \"Negative\": \"#d62728\",  # red\n",
        "    \"Neutral\": \"#d3d3d3\",   # light grey\n",
        "    \"Positive\": \"#1f77b4\"   # blue\n",
        "}\n",
        "\n",
        "# Create a horizontal stacked bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sentiment_distribution.plot(\n",
        "    kind=\"barh\",\n",
        "    stacked=True,\n",
        "    color=[custom_colors[col] for col in sentiment_distribution.columns],  # apply custom colors\n",
        "    alpha=0.85,\n",
        "    figsize=(12, 6)\n",
        ")\n",
        "\n",
        "# Labels and title\n",
        "plt.ylabel(\"Topic\", fontsize=12)\n",
        "plt.xlabel(\"Proportion of Sentiment\", fontsize=12)\n",
        "plt.title(\"Sentiment Distribution by Topics\", fontsize=14)\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title=\"Sentiment Category\", loc=\"best\")\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pCLRH1exViHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Ensure both dictionaries have the same order\n",
        "topics = list(topic_distribution.keys())\n",
        "review_counts = np.array([topic_distribution[topic] for topic in topics])\n",
        "sentiment_scores = np.array([sentiment_by_topic[topic] for topic in topics])\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Define bar positions (y-coordinates)\n",
        "y_pos = np.arange(len(topics))\n",
        "\n",
        "# Plot topic distribution (Blue Bars - Reviews)\n",
        "ax1.barh(y_pos - 0.2, review_counts, color=\"lightblue\", height=0.4, label=\"Number of Reviews\")\n",
        "ax1.set_xlabel(\"Number of Reviews\", color=\"blue\", fontsize=12)\n",
        "ax1.set_ylabel(\"Topic\", fontsize=12)\n",
        "ax1.set_yticks(y_pos)\n",
        "ax1.set_yticklabels(topics)\n",
        "ax1.tick_params(axis=\"x\", labelcolor=\"blue\")\n",
        "\n",
        "# Create second axis for sentiment scores\n",
        "ax2 = ax1.twiny()\n",
        "\n",
        "# Plot sentiment scores (Red Bars - Sentiment)\n",
        "ax2.barh(y_pos + 0.2, sentiment_scores, color=\"lightcoral\", height=0.4, label=\"Average Sentiment\")\n",
        "ax2.set_xlabel(\"Average Sentiment Score\", color=\"red\", fontsize=12)\n",
        "ax2.tick_params(axis=\"x\", labelcolor=\"red\")\n",
        "\n",
        "# Add title\n",
        "plt.title(\"Topic Distribution and Average Sentiment\", fontsize=14)\n",
        "\n",
        "# Add legends at different positions\n",
        "ax1.legend(loc=\"lower right\", fontsize=10)\n",
        "ax2.legend(loc=\"upper right\", fontsize=10)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fHrexupcSunh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "FktJLanTheQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim as gensimvis\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Visualize the topics\n",
        "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "JltCRRXNhlGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "nheQ1yk9d8VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_vader_sentiment(text):\n",
        "    score = analyzer.polarity_scores(str(text))['compound']\n",
        "    if score >= 0.05:\n",
        "        return 'positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['vader_sentiment'] = df['Review Message'].apply(get_vader_sentiment)\n"
      ],
      "metadata": {
        "id": "eMa6h2p1dPox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the manual sentiment annotations into the main dataset (on review text)\n",
        "# Example: Merging on 'review_text' in df_reviews and 'review' in df_annotations\n",
        "df_merged = df.merge(df_annotations[['Review message', 'Annotation']],\n",
        "                             left_on='Review Message', right_on='Review message', how='left')\n"
      ],
      "metadata": {
        "id": "moO-A-OTebeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = df_merged.drop(columns=['Review message'])"
      ],
      "metadata": {
        "id": "fOzQe3_0gzXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head()"
      ],
      "metadata": {
        "id": "2eTPLXpmg5qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign expected sentiment based on annotation or star rating\n",
        "def define_expected_sentiment(row):\n",
        "    if pd.notnull(row['Annotation']):\n",
        "        return row['Annotation']  # Use manual label for 3-star\n",
        "    elif row['Star Rating'] in [1, 2]:\n",
        "        return 'negative'\n",
        "    elif row['Star Rating'] in [4, 5]:\n",
        "        return 'positive'\n",
        "    else:\n",
        "        return 'neutral'  # fallback for unexpected cases\n",
        "\n",
        "df_merged['expected_sentiment'] = df_merged.apply(define_expected_sentiment, axis=1)\n"
      ],
      "metadata": {
        "id": "IEDNa1BOhJbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head()"
      ],
      "metadata": {
        "id": "yoFax7hVhpHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "y_true = df_merged['expected_sentiment']\n",
        "y_pred = df_merged['vader_sentiment']\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Detailed report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['negative', 'neutral', 'positive']))\n"
      ],
      "metadata": {
        "id": "Bbsl2fG-hzd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"y_true labels:\", y_true.unique())\n",
        "print(\"y_pred labels:\", y_pred.unique())"
      ],
      "metadata": {
        "id": "uGNcEURbiZL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define a manual correction map for typos and inconsistencies\n",
        "label_corrections = {\n",
        "    'positive': 'positive',\n",
        "    'Positive': 'positive',\n",
        "    ' negative': 'negative',\n",
        "    'Negative': 'negative',\n",
        "    'negative': 'negative',\n",
        "    ' neutral': 'neutral',\n",
        "    'Neutral': 'neutral',\n",
        "    'neutral': 'neutral',\n",
        "    'Neutal': 'neutral'  # typo correction\n",
        "}\n",
        "\n",
        "# Step 2: Apply correction to y_true (expected_sentiment column)\n",
        "df_merged['expected_sentiment'] = df_merged['expected_sentiment'].map(label_corrections)\n",
        "\n",
        "# Step 3: Filter to keep only valid, non-null rows\n",
        "valid_sentiments = ['negative', 'neutral', 'positive']\n",
        "df_clean = df_merged[\n",
        "    df_merged['expected_sentiment'].isin(valid_sentiments) &\n",
        "    df_merged['vader_sentiment'].isin(valid_sentiments)\n",
        "]\n",
        "\n",
        "# Redefine y_true and y_pred\n",
        "y_true = df_clean['expected_sentiment']\n",
        "y_pred = df_clean['vader_sentiment']\n"
      ],
      "metadata": {
        "id": "rIDJcT0ainRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['negative', 'neutral', 'positive'], zero_division=0))\n"
      ],
      "metadata": {
        "id": "1k9r19RRiqDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch emoji\n"
      ],
      "metadata": {
        "id": "ZovY0HMUnOfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define a lightweight cleaning function\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()             # Remove extra whitespace\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ')    # Flatten line breaks\n",
        "    return text\n",
        "\n",
        "# Apply to your dataset\n",
        "df_merged['Review Message'] = df_clean['Review Message'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "lkYUF5nwoLeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "oajHXHL_np7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Mapping output class to sentiment\n",
        "id2label = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "\n",
        "def batch_bert_sentiment(df_merged, batch_size=32):\n",
        "    texts = list(df_merged['Review Message'].astype(str))  # Ensure strings\n",
        "\n",
        "    # Tokenize all at once\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "\n",
        "    # Move inputs to GPU\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Batch input\n",
        "    dataset = TensorDataset(input_ids, attention_mask)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    # Predict in batches\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            ids, masks = batch\n",
        "            outputs = model(input_ids=ids, attention_mask=masks)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            predictions.extend(preds)\n",
        "\n",
        "    # Convert to sentiment labels\n",
        "    return [id2label[pred] for pred in predictions]\n"
      ],
      "metadata": {
        "id": "alZZ-qNE9Cu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the sentiment prediction — fast and efficient!\n",
        "df_merged['bert_sentiment'] = batch_bert_sentiment(df_merged)"
      ],
      "metadata": {
        "id": "2KKtJya_9fME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"y_true classes:\", sorted(y_true.unique()))\n",
        "print(\"y_pred classes:\", sorted(y_pred.unique()))"
      ],
      "metadata": {
        "id": "y0FJC9DQCbpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and filter invalid entries\n",
        "valid_labels = ['negative', 'neutral', 'positive']\n",
        "\n",
        "# Drop rows with 'nan' (string) or actual NaN in y_true or y_pred\n",
        "df_cleaned = df_merged[\n",
        "    df_merged['expected_sentiment'].astype(str).str.strip().str.lower().isin(valid_labels) &\n",
        "    df_merged['bert_sentiment'].astype(str).str.strip().str.lower().isin(valid_labels)\n",
        "].copy()\n",
        "\n",
        "# Standardize the labels\n",
        "df_cleaned['expected_sentiment'] = df_cleaned['expected_sentiment'].str.strip().str.lower()\n",
        "df_cleaned['bert_sentiment'] = df_cleaned['bert_sentiment'].str.strip().str.lower()\n",
        "\n",
        "# Redefine y_true and y_pred\n",
        "y_true = df_cleaned['expected_sentiment']\n",
        "y_pred = df_cleaned['bert_sentiment']\n"
      ],
      "metadata": {
        "id": "sKlWim5gCmQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    y_true, y_pred,\n",
        "    target_names=['negative', 'neutral', 'positive'],\n",
        "    zero_division=0\n",
        "))\n"
      ],
      "metadata": {
        "id": "a17a-G5WCsTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def batch_bert_sentiment_scores(df_merged, batch_size=32):\n",
        "    texts = list(df_merged['Review Message'].astype(str))  # Ensure strings\n",
        "\n",
        "    # Tokenize all at once\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "\n",
        "    # Move inputs to GPU\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Batch input\n",
        "    dataset = TensorDataset(input_ids, attention_mask)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    # Predict in batches\n",
        "    predictions = []\n",
        "    scores = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            ids, masks = batch\n",
        "            outputs = model(input_ids=ids, attention_mask=masks)\n",
        "            logits = outputs.logits\n",
        "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "            preds = np.argmax(probs, axis=1)\n",
        "            predictions.extend(preds)\n",
        "\n",
        "            # You can pick the confidence of the predicted label\n",
        "            predicted_scores = probs[np.arange(len(probs)), preds]\n",
        "            scores.extend(predicted_scores)\n",
        "\n",
        "    # Convert to sentiment labels\n",
        "    labels = [id2label[pred] for pred in predictions]\n",
        "\n",
        "    return labels, scores\n"
      ],
      "metadata": {
        "id": "mPQuaNQWN-Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run prediction\n",
        "labels, scores = batch_bert_sentiment_scores(df_merged)\n",
        "\n",
        "# Add to your dataframe\n",
        "df_merged['bert_sentiment'] = labels\n",
        "df_merged['bert_sentiment_score'] = scores\n"
      ],
      "metadata": {
        "id": "kbQv4kJROBpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head(20)"
      ],
      "metadata": {
        "id": "5OZEFL8DSxNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_negative = df[df['Star Rating'].isin([1, 2, 3])]"
      ],
      "metadata": {
        "id": "0l7NmJ3JlzjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all review messages into one string\n",
        "text_negative = \" \".join(review for review in df_negative['Review Message'].astype(str))\n",
        "\n",
        "# Define additional stopwords if needed\n",
        "custom_stopwords = set(STOPWORDS)\n",
        "\n",
        "# Create the word cloud\n",
        "wordcloud_negative = WordCloud(width=800, height=400,\n",
        "                               background_color='white',\n",
        "                               stopwords=custom_stopwords,\n",
        "                               colormap='Reds').generate(text_negative)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of Negative Reviews – Tesco Bank\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ay0u4MfN_Bdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_negative.head()"
      ],
      "metadata": {
        "id": "3E_s1FfHncbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create corpus for df_negative using the existing id2word dictionary\n",
        "corpus_negative = [id2word.doc2bow(text) for text in df_negative['cleaned_text']]\n"
      ],
      "metadata": {
        "id": "o6ENx7TQoRDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "lda_model_negative = LdaModel(corpus=corpus_negative,\n",
        "                              id2word=id2word,\n",
        "                              num_topics=3,  # or however many you want\n",
        "                              random_state=42,\n",
        "                              passes=10,\n",
        "                              per_word_topics=True)\n",
        "\n",
        "# Display topics\n",
        "for idx, topic in lda_model_negative.print_topics(-1):\n",
        "    print(f\"Topic {idx}: {topic}\")"
      ],
      "metadata": {
        "id": "dpUw8SXZqIHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_negative = CoherenceModel(\n",
        "    model=lda_model_negative,\n",
        "    texts=df_negative['cleaned_text'],  # this should be tokenized text (list of words)\n",
        "    dictionary=id2word,\n",
        "    coherence='c_v'  # 'c_v' is a popular choice, but others are available\n",
        ")\n",
        "\n",
        "coherence_score = coherence_model_negative.get_coherence()\n",
        "print(f\"\\nCoherence Score: {coherence_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "keUHkSj7u0k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels = {\n",
        "    0: \"Credit card Issues\",\n",
        "    1: \"Customer Service Complaints\",\n",
        "    2: \"Insurance Claims & Policy Issues\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "NOqICpzHv2XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_negative.columns)\n"
      ],
      "metadata": {
        "id": "t6oYShYe6yPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_labels = {\n",
        "    0: \"Credit card Issues\",\n",
        "    1: \"Customer Service Complaints\",\n",
        "    2: \"Insurance Claims & Policy Issues\"\n",
        "}\n",
        "\n",
        "# 2. Function to get dominant topic index\n",
        "def get_dominant_topic(ldamodel, corpus):\n",
        "    dominant_topics = []\n",
        "    for bow in corpus:\n",
        "        topic_probs = ldamodel.get_document_topics(bow)\n",
        "        if topic_probs:\n",
        "            dominant_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
        "            dominant_topics.append(dominant_topic)\n",
        "        else:\n",
        "            dominant_topics.append(None)\n",
        "    return dominant_topics\n",
        "\n",
        "# 3. Apply to negative reviews\n",
        "df_negative['Theme'] = get_dominant_topic(lda_model_negative, corpus_negative)\n",
        "\n",
        "# 4. Create final DataFrame with the columns you want\n",
        "df_negative_selected = df_negative[['cleaned_text', 'Sentiment Score', 'Theme']].copy()\n",
        "\n",
        "# 5. Map the topic labels as 'Theme Label'\n",
        "df_negative_selected['Theme Label'] = df_negative_selected['Theme'].map(topic_labels)\n",
        "\n",
        "# 6. Done! Preview your data\n",
        "print(df_negative_selected.head())\n"
      ],
      "metadata": {
        "id": "nyFsplryAj7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_negative.loc[:, 'Theme Label'] = df_negative['Theme'].map(topic_labels)"
      ],
      "metadata": {
        "id": "D_TPEUqqUUkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Select the key columns\n",
        "df_negative_selected = df_negative[['cleaned_text', 'Sentiment Score', 'Theme Label']].copy()"
      ],
      "metadata": {
        "id": "9cNd6sUKAoYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_negative_selected.head()"
      ],
      "metadata": {
        "id": "OxgquZXMU_vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Join all words in cleaned_text into a single string\n",
        "text = ' '.join([' '.join(words) for words in df_negative_selected['cleaned_text'] if isinstance(words, list)])\n",
        "\n",
        "# Create and display the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud for Negative Reviews\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ejycUSKfVSxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of reviews per Theme Label\n",
        "theme_counts = df_negative_selected['Theme Label'].value_counts().reset_index()\n",
        "theme_counts.columns = ['Theme Label', 'Count']\n",
        "print(theme_counts)\n",
        "\n",
        "# Horizontal bar chart with red bars\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(theme_counts['Theme Label'], theme_counts['Count'], color='red')\n",
        "plt.title('Number of Reviews per Theme Label')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Theme Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eP0dAqvqWMVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV file\n",
        "df_store = pd.read_csv('/content/drive/MyDrive/MSc Dissertation/Tescostore (2).csv')"
      ],
      "metadata": {
        "id": "LWhf0kaNX8HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store.head()"
      ],
      "metadata": {
        "id": "37gVKLu8Yck5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store['Review Date'] = df_store['Review Date'].str.replace('Date of experience: ', '').str.strip()\n",
        "df_store['Star Rating'] = df_store['Star Rating'].str.extract(r'(\\d+)').astype('Int64')"
      ],
      "metadata": {
        "id": "OZBNyuqnYeUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store['Review Date'] = pd.to_datetime(df_store['Review Date'], errors='coerce')\n"
      ],
      "metadata": {
        "id": "9xRhjrGTZB4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store = df_store.dropna(subset=['Review Message'])"
      ],
      "metadata": {
        "id": "yzjSWMuQZePM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "df_store = df_store.drop_duplicates()"
      ],
      "metadata": {
        "id": "Pog1OcFyZoCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotations['Date'] = pd.to_datetime(df_annotations['Date'], errors='coerce')"
      ],
      "metadata": {
        "id": "x8RuW2nDZRAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store['Author Name'] = df_store['Author Name'].fillna('Anonymous')"
      ],
      "metadata": {
        "id": "WGuz2EwxZybc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = df_store[df_store['Review Date'].dt.year.between(2020, 2025)]"
      ],
      "metadata": {
        "id": "KMZMAfWhaHcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store.info()"
      ],
      "metadata": {
        "id": "NVwToBLzaUc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show rows where Review Date is missing\n",
        "print(df_store[df_store['Review Date'].isna()])\n",
        "\n",
        "# Show rows where Star Rating is missing\n",
        "print(df_store[df_store['Star Rating'].isna()])\n"
      ],
      "metadata": {
        "id": "-4lyMVVwaYuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop row with index 12237\n",
        "df_store = df_store.drop(index=12237).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "IZGj-qo3bJef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_store['Star Rating'].value_counts().sort_index().plot(\n",
        "    kind='bar', color='orange', edgecolor='black'\n",
        ")\n",
        "plt.title('Tesco Stores - Distribution of Star Ratings')\n",
        "plt.xlabel('Star Rating')\n",
        "plt.ylabel('Count of Reviews')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "px9ygryobK95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rate_to_sentiment(star):\n",
        "    if star <= 2:\n",
        "        return 'Negative'\n",
        "    elif star == 3:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Positive'\n",
        "\n",
        "df_store['Sentiment Label'] = df_store['Star Rating'].apply(rate_to_sentiment)\n",
        "\n"
      ],
      "metadata": {
        "id": "rR01iznyb_FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot sentiment label distribution\n",
        "df_store['Sentiment Label'].value_counts().plot(\n",
        "    kind='bar',\n",
        "    color=['red', 'green', 'gray'],  # red = Negative, gray = Neutral, green = Positive\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.title('Tesco Stores - Sentiment Distribution from Star Ratings')\n",
        "plt.xlabel('Sentiment Label')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bbKCpYJ7b9ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning function\n",
        "def clean_text(text):\n",
        "    doc = nlp(str(text).lower())  # Ensure text is string\n",
        "    return [\n",
        "        token.lemma_ for token in doc\n",
        "        if token.is_alpha and not token.is_stop and len(token) > 2\n",
        "    ]\n",
        "\n",
        "# Apply cleaning\n",
        "df_store['cleaned_text'] = df_store['Review Message'].apply(clean_text)\n",
        "\n",
        "# Inspect cleaned data\n",
        "print(df_store[['Review Message', 'cleaned_text']].head())"
      ],
      "metadata": {
        "id": "z0YoEBSBdsYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and lowercase, remove punctuation\n",
        "tokenized_texts = df_store['Review Message'].dropna().apply(\n",
        "    lambda x: simple_preprocess(str(x), deacc=True)\n",
        ").tolist()\n"
      ],
      "metadata": {
        "id": "t_ov0cUfitDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build bigram model\n",
        "bigram = Phrases(df_store['cleaned_text'], min_count=5, threshold=10)\n",
        "bigram_mod = Phraser(bigram)\n",
        "\n",
        "# Apply bigram model\n",
        "df_store['cleaned_text'] = df_store['cleaned_text'].apply(lambda text: bigram_mod[text])"
      ],
      "metadata": {
        "id": "kX2EEo3geHnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download essential tokenizers\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "I6feixkjiYOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Apply stopword filtering\n",
        "tokenized_texts = df_store['Review Message'].dropna().apply(\n",
        "    lambda x: [word for word in simple_preprocess(str(x), deacc=True) if word not in stop_words]\n",
        ").tolist()\n"
      ],
      "metadata": {
        "id": "oJ6wGBVli19D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build the bigram and trigram models\n",
        "bigram = Phrases(tokenized_texts, min_count=5, threshold=100)\n",
        "trigram = Phrases(bigram[tokenized_texts], threshold=100)\n",
        "\n",
        "# Faster transformation\n",
        "bigram_mod = Phraser(bigram)\n",
        "trigram_mod = Phraser(trigram)\n",
        "\n",
        "# Apply bigrams and trigrams\n",
        "texts_bigrams = [bigram_mod[doc] for doc in tokenized_texts]\n",
        "texts_trigrams = [trigram_mod[bigram_mod[doc]] for doc in tokenized_texts]\n"
      ],
      "metadata": {
        "id": "NVGeDVZ3i4rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize while keeping only nouns, adjectives, verbs, adverbs\n",
        "def lemmatize_texts(texts):\n",
        "    lemmatized_texts = []\n",
        "    for doc in texts:\n",
        "        joined = ' '.join(doc)  # Convert list to string\n",
        "        spacy_doc = nlp(joined)\n",
        "        lemmatized_texts.append([token.lemma_ for token in spacy_doc if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']])\n",
        "    return lemmatized_texts\n",
        "\n",
        "texts_lemmatized = lemmatize_texts(texts_trigrams)\n"
      ],
      "metadata": {
        "id": "vreTcQYflPHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Create Dictionary and Corpus for Tesco Stores\n",
        "id2word_stores = Dictionary(texts_lemmatized)\n",
        "corpus_stores = [id2word_stores.doc2bow(text) for text in texts_lemmatized]\n"
      ],
      "metadata": {
        "id": "ifkk_ijLlnEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "lda_model_stores = LdaModel(\n",
        "    corpus=corpus_stores,\n",
        "    id2word=id2word_stores,\n",
        "    num_topics=6,               # You can still tune this\n",
        "    random_state=42,\n",
        "    update_every=1,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "ngaq6yeMmD-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = lda_model_stores.print_topics(num_words=10)\n",
        "for idx, topic in topics:\n",
        "    print(f\"Theme {idx}: {topic}\")\n"
      ],
      "metadata": {
        "id": "4mDZs5Njmm0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "coherence_model_stores = CoherenceModel(\n",
        "    model=lda_model_stores,\n",
        "    texts=texts_lemmatized,   # your final tokenized store reviews\n",
        "    dictionary=id2word,       # if you've renamed it, use id2word_stores\n",
        "    coherence='c_v'\n",
        ")\n",
        "coherence_score_stores = coherence_model_stores.get_coherence()\n",
        "print(f\"\\nTesco Stores Coherence Score: {coherence_score_stores:.4f}\")\n"
      ],
      "metadata": {
        "id": "_YjlbDuNmuM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_themes = {\n",
        "0: \"Delivery Problem\",\n",
        "1: \"Product Quality and Value\",\n",
        "2: \"Shopping at Tesco/Billing\",\n",
        "3: \"Customer Service Interactions\",\n",
        "4: \"In-Store Experience\",\n",
        "5: \"Alternative Options and Comparisons\"\n",
        "}"
      ],
      "metadata": {
        "id": "4ze1JuaCFP2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model_stores.print_topics():\n",
        "    print(f\"Theme {idx} - {store_themes[idx]}: {topic}\\n\")\n"
      ],
      "metadata": {
        "id": "XaVrftHzHR77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Step 1: Apply sentiment analysis to each review\n",
        "df_store[\"Sentiment Score\"] = df_store[\"Review Message\"].apply(\n",
        "    lambda text: sia.polarity_scores(text)[\"compound\"]\n",
        ")\n",
        "\n",
        "# Step 2: Assign topic distribution from the LDA model\n",
        "df_store[\"Topic\"] = [\n",
        "    lda_model_stores.get_document_topics(corpus_stores[i], minimum_probability=0.3)\n",
        "    for i in range(len(df_store))\n",
        "]\n",
        "\n",
        "# Step 3: Extract dominant topic and map to store theme label\n",
        "df_store[\"Dominant Topic\"] = df_store[\"Topic\"].apply(\n",
        "    lambda topics: store_themes[max(topics, key=lambda x: x[1])[0]] if topics else None\n",
        ")\n",
        "\n",
        "# Step 4: Calculate average sentiment per topic\n",
        "sentiment_by_topic_store = df_store.groupby(\"Dominant Topic\")[\"Sentiment Score\"].mean()\n",
        "\n",
        "# Display result\n",
        "print(\"\\n✅ Average Sentiment Score per Tesco Store Topic:\")\n",
        "print(sentiment_by_topic_store)\n"
      ],
      "metadata": {
        "id": "71oD_fZYKY5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sort the sentiment scores\n",
        "sentiment_by_topic_store = sentiment_by_topic_store.sort_values(ascending=True)\n",
        "\n",
        "# Generate color list based on sentiment polarity\n",
        "colors = ['green' if score >= 0 else 'red' for score in sentiment_by_topic_store.values]\n",
        "\n",
        "# Create the horizontal bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    x=sentiment_by_topic_store.values,\n",
        "    y=sentiment_by_topic_store.index,\n",
        "    palette=colors\n",
        ")\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Average Sentiment Score\")\n",
        "plt.ylabel(\"Tesco Store Theme\")\n",
        "plt.title(\"Sentiment Score per Topic – Tesco Stores\")\n",
        "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)  # Neutral reference line\n",
        "\n",
        "# Clean up\n",
        "plt.legend([], [], frameon=False)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2BYI5GQJPChI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Count number of reviews per dominant topic\n",
        "topic_distribution_store = df_store[\"Dominant Topic\"].value_counts()\n",
        "\n",
        "# Create a horizontal bar chart with blue bars\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    x=topic_distribution_store.values,\n",
        "    y=topic_distribution_store.index,\n",
        "    color=\"orange\"\n",
        ")\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Number of Reviews\")\n",
        "plt.ylabel(\"Tesco Store Theme\")\n",
        "plt.title(\"Distribution of Topics – Tesco Stores\")\n",
        "\n",
        "# Layout and show\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "og3FobH6QFNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all Tesco Stores reviews into a single string\n",
        "all_reviews_store_text = \" \".join(df_store[\"Review Message\"].astype(str))\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud_store = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color=\"white\",\n",
        "    colormap=\"viridis\"\n",
        ").generate(all_reviews_store_text)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_store, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of Tesco Store Reviews\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_G2apZMrQplD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentiment(score):\n",
        "    if score >= 0.3:\n",
        "        return \"Positive\"\n",
        "    elif score <= -0.1:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "df_store[\"Sentiment Category\"] = df_store[\"Sentiment Score\"].apply(classify_sentiment)\n"
      ],
      "metadata": {
        "id": "ubdM28cBUWDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalculate sentiment distribution per topic\n",
        "sentiment_distribution_store = (\n",
        "    df_store.groupby(\"Dominant Topic\")[\"Sentiment Category\"]\n",
        "    .value_counts(normalize=True)\n",
        "    .unstack()\n",
        "    .fillna(0)\n",
        ")\n",
        "\n",
        "# Ensure column order\n",
        "sentiment_distribution_store = sentiment_distribution_store[[\"Negative\", \"Neutral\", \"Positive\"]]\n",
        "\n",
        "# Plot\n",
        "sentiment_distribution_store.plot(\n",
        "    kind=\"barh\",\n",
        "    stacked=True,\n",
        "    color=[\"red\", \"lightgrey\", \"green\"],  # manual mapping\n",
        "    alpha=0.85,\n",
        "    figsize=(12, 6)\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Proportion of Sentiment\")\n",
        "plt.ylabel(\"Tesco Store Topic\")\n",
        "plt.title(\"Sentiment Distribution by Topic – Tesco Stores (Recalibrated)\")\n",
        "plt.legend(title=\"Sentiment Category\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UdmnxMwaUXcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Prepare matching topic order\n",
        "topics_store = list(sentiment_by_topic_store.index)  # topics in sentiment order\n",
        "\n",
        "# Ensure the same topic order for review counts\n",
        "review_counts_store = np.array([df_store[\"Dominant Topic\"].value_counts().get(topic, 0) for topic in topics_store])\n",
        "sentiment_scores_store = np.array([sentiment_by_topic_store[topic] for topic in topics_store])\n",
        "\n",
        "# Step 2: Create plot\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Bar positions\n",
        "y_pos = np.arange(len(topics_store))\n",
        "\n",
        "# Plot review counts (Blue bars)\n",
        "ax1.barh(y_pos - 0.2, review_counts_store, color=\"lightblue\", height=0.4, label=\"Number of Reviews\")\n",
        "ax1.set_xlabel(\"Number of Reviews\", color=\"blue\", fontsize=12)\n",
        "ax1.set_ylabel(\"Tesco Store Topic\", fontsize=12)\n",
        "ax1.set_yticks(y_pos)\n",
        "ax1.set_yticklabels(topics_store)\n",
        "ax1.tick_params(axis=\"x\", labelcolor=\"blue\")\n",
        "\n",
        "# Plot sentiment scores (Red bars on second x-axis)\n",
        "ax2 = ax1.twiny()\n",
        "ax2.barh(y_pos + 0.2, sentiment_scores_store, color=\"lightcoral\", height=0.4, label=\"Average Sentiment\")\n",
        "ax2.set_xlabel(\"Average Sentiment Score\", color=\"red\", fontsize=12)\n",
        "ax2.tick_params(axis=\"x\", labelcolor=\"red\")\n",
        "\n",
        "# Title and legends\n",
        "plt.title(\"Topic Distribution and Average Sentiment – Tesco Stores\", fontsize=14)\n",
        "ax1.legend(loc=\"lower right\", fontsize=10)\n",
        "ax2.legend(loc=\"upper right\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2U2XUT5GUnKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Annotated dataset for 3-star reviews tesco bank\n",
        "dfstore_annotations = pd.read_csv('/content/drive/MyDrive/MSc Dissertation/Tesco Stores Review (1).csv')  # should have columns like: review_text, annotated_sentiment\n",
        "\n",
        "\n",
        "# Inspect the data\n",
        "print(dfstore_annotations.head())\n"
      ],
      "metadata": {
        "id": "x2IcmszqgE8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store = df_store.merge(\n",
        "    dfstore_annotations[['Review', 'Annotation']],\n",
        "    left_on='Review Message',\n",
        "    right_on='Review',\n",
        "    how='left'\n",
        ")\n"
      ],
      "metadata": {
        "id": "kWR-xq-kihhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store.drop(columns='Review', inplace=True)"
      ],
      "metadata": {
        "id": "2IJk6aj-ipzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store.head()"
      ],
      "metadata": {
        "id": "t6Gp1pDriroa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_expected_sentiment(row):\n",
        "    if pd.notnull(row['Annotation']):\n",
        "        return row['Annotation'].capitalize()\n",
        "    elif row['Star Rating'] in [1, 2]:\n",
        "        return 'Negative'\n",
        "    elif row['Star Rating'] in [4, 5]:\n",
        "        return 'Positive'\n",
        "    else:\n",
        "        return 'Neutral'  # For 3-star reviews without annotations\n"
      ],
      "metadata": {
        "id": "v9bwSONOjl_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store['Expected Sentiment'] = df_store.apply(define_expected_sentiment, axis=1)"
      ],
      "metadata": {
        "id": "lUV3eCC5ks3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store.head()"
      ],
      "metadata": {
        "id": "Oies6_Eijr7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Step 1: Set up ground truth and predicted labels\n",
        "y_true = df_store['Expected Sentiment']\n",
        "y_pred = df_store['Sentiment Category']  # VADER's predicted category\n",
        "\n",
        "# Step 2: Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "# Step 3: Display metrics\n",
        "print(f\"✅ Accuracy: {accuracy:.2f}\")\n",
        "print(f\"✅ Precision: {precision:.2f}\")\n",
        "print(f\"✅ Recall: {recall:.2f}\")\n",
        "print(f\"✅ F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Step 4: Detailed classification report\n",
        "print(\"\\n📊 Classification Report:\")\n",
        "print(classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    labels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "    target_names=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "    zero_division=0\n",
        "))\n"
      ],
      "metadata": {
        "id": "JF8xjIW1lKeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted Sentiment\")\n",
        "plt.ylabel(\"True (Expected) Sentiment\")\n",
        "plt.title(\"Confusion Matrix – VADER vs Expected Sentiment (Tesco Stores)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-zb2Z8W9ll4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch emoji"
      ],
      "metadata": {
        "id": "JTdprhI1mAoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define lightweight cleaning function\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()             # Remove extra whitespace\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ')    # Flatten line breaks\n",
        "    return text\n",
        "\n",
        "# Apply to Tesco Store dataset\n",
        "df_store['Review Message'] = df_store['Review Message'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "kPsptpCsm3iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_bert_sentiment(texts, batch_size=32):\n",
        "    sentiments = []\n",
        "    model.eval()\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "        batch_sentiments = [labels[p] for p in preds]\n",
        "        sentiments.extend(batch_sentiments)\n",
        "\n",
        "    return sentiments\n"
      ],
      "metadata": {
        "id": "x0K-qA_XnwDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Force use of CPU\n",
        "device = torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define label map\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n"
      ],
      "metadata": {
        "id": "MPLr5QFIob8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store['RoBERTa Sentiment'] = batch_bert_sentiment(df_store['Review Message'].tolist())"
      ],
      "metadata": {
        "id": "vZZZ9hSxqm5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "id": "uGSaOi1cvhi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_bert_sentiment(texts, batch_size=8):  # reduce batch size\n",
        "    sentiments = []\n",
        "    model.eval()\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        print(f\"Processing batch {i}-{i+batch_size}...\")  # track progress\n",
        "\n",
        "        try:\n",
        "            inputs = tokenizer(\n",
        "                batch_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "                preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            batch_sentiments = [labels[p] for p in preds]\n",
        "            sentiments.extend(batch_sentiments)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error in batch {i}-{i+batch_size}: {e}\")\n",
        "            sentiments.extend([None]*len(batch_texts))\n",
        "\n",
        "    return sentiments\n"
      ],
      "metadata": {
        "id": "B5XSzlZkwNR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_store.head(50).copy()\n",
        "df_test['RoBERTa Sentiment'] = batch_bert_sentiment(df_test['Review Message'].tolist())\n"
      ],
      "metadata": {
        "id": "gUejC48JwQxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store predictions here\n",
        "roberta_sentiments = []\n",
        "\n",
        "# Define batch size (adjust if needed)\n",
        "batch_size = 64\n",
        "\n",
        "# Total number of reviews\n",
        "texts = df_store['Review Message'].dropna().tolist()\n",
        "total = len(texts)\n",
        "\n",
        "# Process in batches\n",
        "for i in range(0, total, batch_size):\n",
        "    print(f\"Processing batch {i} to {min(i + batch_size, total)}...\")\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    batch_preds = batch_bert_sentiment(batch_texts)\n",
        "    roberta_sentiments.extend(batch_preds)\n"
      ],
      "metadata": {
        "id": "F5-L_hHPxqkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the results as a new column\n",
        "df_store = df_store.copy()\n",
        "df_store.loc[:len(roberta_sentiments)-1, 'RoBERTa Sentiment'] = roberta_sentiments\n"
      ],
      "metadata": {
        "id": "0IlTvNIvMGvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store[['Review Message', 'Expected Sentiment', 'RoBERTa Sentiment']].head()\n"
      ],
      "metadata": {
        "id": "mOhT6IfgM6JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_bert_sentiment_scores(texts, batch_size=32):\n",
        "    all_scores = []\n",
        "    model.eval()\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        print(f\"Processing batch {i}-{i+batch_size}...\")\n",
        "\n",
        "        try:\n",
        "            inputs = tokenizer(\n",
        "                batch_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "\n",
        "            # Assign sentiment score: Negative = -1, Neutral = 0, Positive = +1\n",
        "            weights = torch.tensor([-1, 0, 1], device=device).float()\n",
        "            scores = (probs * weights).sum(dim=1).tolist()  # weighted average\n",
        "\n",
        "            all_scores.extend(scores)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error in batch {i}-{i+batch_size}: {e}\")\n",
        "            all_scores.extend([None] * len(batch_texts))\n",
        "\n",
        "    return all_scores\n"
      ],
      "metadata": {
        "id": "ViWhrgMmOKMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run it on all reviews\n",
        "sentiment_score_list = batch_bert_sentiment_scores(df_store['Review Message'].tolist())\n",
        "\n",
        "# Add to df_store\n",
        "df_store['RoBERTa Sentiment Score'] = sentiment_score_list\n"
      ],
      "metadata": {
        "id": "Xa-x8JtoOUXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_store.head()"
      ],
      "metadata": {
        "id": "JhR7USXu7MWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by Dominant Topic and calculate average RoBERTa sentiment score\n",
        "avg_sentiment_per_topic = df_store.groupby(\"Dominant Topic\")[\"RoBERTa Sentiment Score\"].mean().sort_values()\n",
        "\n",
        "# Display as a neat table\n",
        "import pandas as pd\n",
        "pd.set_option('display.float_format', '{:.3f}'.format)  # format float nicely\n",
        "avg_sentiment_per_topic = avg_sentiment_per_topic.reset_index()\n",
        "avg_sentiment_per_topic.columns = ['Topic', 'Average RoBERTa Sentiment Score']\n",
        "avg_sentiment_per_topic\n"
      ],
      "metadata": {
        "id": "KH9KVGaK8ISz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate RoBERTa predictions\n",
        "y_true = df_store['Expected Sentiment']\n",
        "y_pred = df_store['RoBERTa Sentiment']\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"✅ RoBERTa Accuracy: {accuracy:.2f}\")\n",
        "print(f\"✅ RoBERTa Precision: {precision:.2f}\")\n",
        "print(f\"✅ RoBERTa Recall: {recall:.2f}\")\n",
        "print(f\"✅ RoBERTa F1 Score: {f1:.2f}\")\n",
        "\n",
        "print(\"\\n📊 RoBERTa Classification Report:\")\n",
        "print(classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    labels=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "    target_names=[\"Negative\", \"Neutral\", \"Positive\"],\n",
        "    zero_division=0\n",
        "))\n"
      ],
      "metadata": {
        "id": "bA46mwyo82aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define labels for consistency\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(df_store['Expected Sentiment'], df_store['RoBERTa Sentiment'], labels=labels)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\", xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted Sentiment (RoBERTa)\")\n",
        "plt.ylabel(\"Actual Sentiment (Expected)\")\n",
        "plt.title(\"Confusion Matrix – RoBERTa vs Expected Sentiment - Tesco Stores\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ezr3Jj309Nkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Recalculate average sentiment per topic\n",
        "avg_sentiment = df_store.groupby(\"Dominant Topic\")[\"RoBERTa Sentiment Score\"].mean().sort_values()\n",
        "\n",
        "# Define colors based on score\n",
        "colors = ['green' if val > 0 else 'red' for val in avg_sentiment]\n",
        "\n",
        "# Plot with conditional colors\n",
        "plt.figure(figsize=(10, 6))\n",
        "avg_sentiment.plot(kind=\"barh\", color=colors)\n",
        "plt.xlabel(\"Average RoBERTa Sentiment Score\")\n",
        "plt.ylabel(\"Tesco Store Topic\")\n",
        "plt.title(\"Average Sentiment Score per Topic – Tesco Stores\")\n",
        "plt.axvline(0, color='black', linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DvqIca4u_CP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Ensure column order\n",
        "sentiment_distribution_store = sentiment_distribution_store[[\"Negative\", \"Neutral\", \"Positive\"]]\n",
        "\n",
        "# Step 3: Plot stacked horizontal bar chart\n",
        "sentiment_distribution_store.plot(\n",
        "    kind=\"barh\",\n",
        "    stacked=True,\n",
        "    color=[\"red\", \"lightgrey\", \"green\"],  # Custom sentiment colors\n",
        "    alpha=0.85,\n",
        "    figsize=(12, 6)\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Proportion of Sentiment\")\n",
        "plt.ylabel(\"Tesco Store Topic\")\n",
        "plt.title(\"Sentiment Distribution by Topic – Tesco Stores (RoBERTa)\")\n",
        "plt.legend(title=\"Sentiment Category\", loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "flyzMII4_MOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Make sure average sentiment per topic is calculated\n",
        "sentiment_by_topic_store = df_store.groupby(\"Dominant Topic\")[\"RoBERTa Sentiment Score\"].mean().sort_values()\n",
        "\n",
        "# Step 2: Match topic order across both axes\n",
        "topics_store = list(sentiment_by_topic_store.index)\n",
        "review_counts_store = np.array([df_store[\"Dominant Topic\"].value_counts().get(topic, 0) for topic in topics_store])\n",
        "sentiment_scores_store = np.array([sentiment_by_topic_store[topic] for topic in topics_store])\n",
        "\n",
        "# Step 3: Create dual-axis plot\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "y_pos = np.arange(len(topics_store))\n",
        "\n",
        "# Left axis: Number of reviews (blue bars)\n",
        "ax1.barh(y_pos - 0.2, review_counts_store, color=\"lightblue\", height=0.4, label=\"Number of Reviews\")\n",
        "ax1.set_xlabel(\"Number of Reviews\", color=\"blue\", fontsize=12)\n",
        "ax1.set_ylabel(\"Tesco Store Topic\", fontsize=12)\n",
        "ax1.set_yticks(y_pos)\n",
        "ax1.set_yticklabels(topics_store)\n",
        "ax1.tick_params(axis=\"x\", labelcolor=\"blue\")\n",
        "\n",
        "# Right axis: Average sentiment score (red bars)\n",
        "ax2 = ax1.twiny()\n",
        "ax2.barh(y_pos + 0.2, sentiment_scores_store, color=\"lightcoral\", height=0.4, label=\"Average Sentiment\")\n",
        "ax2.set_xlabel(\"Average Sentiment Score\", color=\"red\", fontsize=12)\n",
        "ax2.tick_params(axis=\"x\", labelcolor=\"red\")\n",
        "\n",
        "# Add title and legends\n",
        "plt.title(\"Topic Distribution and Average Sentiment – Tesco Stores\", fontsize=14)\n",
        "ax1.legend(loc=\"lower right\", fontsize=10)\n",
        "ax2.legend(loc=\"upper right\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i8fuXKC7Atz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter negative reviews\n",
        "df_negative_store = df_store[df_store['Star Rating'].isin([1, 2, 3])].copy()\n",
        "\n",
        "# If you have trigrammed tokens for the full dataset, do the same for negatives\n",
        "texts_trigrams_neg = df_negative_store[\"cleaned_text\"]\n"
      ],
      "metadata": {
        "id": "ITvQn1P5CKhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_lemmatized_neg = lemmatize_texts(texts_trigrams_neg)"
      ],
      "metadata": {
        "id": "xyJghauICOfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Create Dictionary and Corpus\n",
        "id2word_neg = Dictionary(texts_lemmatized_neg)\n",
        "corpus_neg = [id2word_neg.doc2bow(text) for text in texts_lemmatized_neg]\n",
        "\n",
        "# Train LDA model\n",
        "lda_model_neg = LdaModel(\n",
        "    corpus=corpus_neg,\n",
        "    id2word=id2word_neg,\n",
        "    num_topics=3,  # You can tune this!\n",
        "    random_state=42,\n",
        "    update_every=1,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "uPYlNg0dCN_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model_neg.print_topics(num_words=10):\n",
        "    print(f\"Topic {idx}: {topic}\")"
      ],
      "metadata": {
        "id": "Zxy-JbVYDXSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Build coherence model\n",
        "coherence_model_neg = CoherenceModel(\n",
        "    model=lda_model_neg,\n",
        "    texts=texts_lemmatized_neg,\n",
        "    dictionary=id2word_neg,\n",
        "    coherence='c_v'  # You can also try 'u_mass' or 'c_uci' for comparison\n",
        ")\n",
        "\n",
        "# Compute the coherence score\n",
        "coherence_score = coherence_model_neg.get_coherence()\n",
        "print(f\"\\n✅ Coherence Score (c_v): {coherence_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "ve_VStVhDbnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_themes = {\n",
        "0: \"Delivery and Order\",\n",
        "1: \"In-Store Experience\",\n",
        "2: \"Product and Pricing\"\n",
        "}"
      ],
      "metadata": {
        "id": "iDaqmC2qE-5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign dominant topic using the LDA model for negative reviews\n",
        "def get_dominant_topic(ldamodel, corpus):\n",
        "    dominant_topics = []\n",
        "    for bow in corpus:\n",
        "        topics = ldamodel.get_document_topics(bow)\n",
        "        if topics:\n",
        "            dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
        "        else:\n",
        "            dominant_topic = None\n",
        "        dominant_topics.append(dominant_topic)\n",
        "    return dominant_topics\n",
        "\n",
        "# Assign topics to the negative dataset\n",
        "df_negative_store[\"LDA Topic\"] = get_dominant_topic(lda_model_neg, corpus_neg)\n"
      ],
      "metadata": {
        "id": "TySXmdVbGZOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map LDA Topic to human-readable theme\n",
        "df_negative_store[\"Complaint Theme\"] = df_negative_store[\"LDA Topic\"].map(negative_themes)\n",
        "\n",
        "# Compute average sentiment score per complaint theme\n",
        "avg_sentiment_per_theme = (\n",
        "    df_negative_store.groupby(\"Complaint Theme\")[\"RoBERTa Sentiment Score\"]\n",
        "    .mean()\n",
        "    .sort_values()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Display table\n",
        "avg_sentiment_per_theme.columns = ['Complaint Theme', 'Average Sentiment Score']\n",
        "avg_sentiment_per_theme.style.format({'Average Sentiment Score': '{:.4f}'})\n"
      ],
      "metadata": {
        "id": "tFwI59ybGfxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}